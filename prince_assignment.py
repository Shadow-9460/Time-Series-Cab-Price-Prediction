# -*- coding: utf-8 -*-
"""Prince Assignment

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X0_eAPaV-LOU7RGzxdW66KqBDjvIE9SI
"""

from google.colab import drive
drive.mount('/content/drive')

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.statespace.sarimax import SARIMAX

import os
import pandas as pd
from glob import glob

# Specify the folder path
folder_path = '/content/drive/MyDrive/Maiero Mobility'

# Use glob to get a list of all JSON files in the folder
files = glob(os.path.join(folder_path, '*.json'))  # Change the file extension as needed

# Create an empty DataFrame to store the combined data
combined_data = pd.DataFrame()

# Iterate through the files and read them into Pandas DataFrames
for file in files:
    df = pd.read_json(file)  # Adjust the reading function based on the file format
    combined_data = combined_data.append(df, ignore_index=True)

# Now combined_data contains the data from all JSON files in the folder

df = combined_data

combined_data.tail(4)

"""## EDA"""

# Analyze Trip distance, Trip Duration using Histplot To find the peak using Histplot

df['tripDistance'] = pd.to_numeric(df['tripDistance'])
df['tripSpeed'] = pd.to_numeric(df['tripSpeed'])
df['tripDuration'] = pd.to_numeric(df['tripDuration'])

# Plotting Distribution of Trip Distances
plt.figure(figsize=(15, 4))
plt.subplot(1, 3, 1)
df['tripDistance'].plot(kind='hist', bins=20, edgecolor='black')
plt.title('Distribution of Trip Distances')
plt.xlabel('Trip Distance (km)')
plt.ylabel('Frequency')

# Plotting Distribution of Trip Speeds
plt.subplot(1, 3, 2)
df['tripSpeed'].plot(kind='hist', bins=20, edgecolor='black')
plt.title('Distribution of Trip Speeds')
plt.xlabel('Trip Speed (km/h)')
plt.ylabel('Frequency')

# Plotting the Distribution of Trip Durations
plt.subplot(1, 3, 3)
df['tripDuration'].plot(kind='hist', bins=20, edgecolor='black')
plt.title('Distribution of Trip Durations')
plt.xlabel('Trip Duration (minutes)')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

"""### 1.From analysis we find that maximum trip distance lies between 3 to 11 km
### 2. From analysis we find that trip speed will be 18 to 26 so city might be crowded
### 3. Max trip duration will lies between 10 to 30 min
"""

# Distribution of UPI and Cash
payment_distribution = df['paymentType'].value_counts()

plt.figure(figsize=(8, 8))
plt.pie(payment_distribution, labels=payment_distribution.index, autopct='%1.1f%%', startangle=90, colors=['skyblue', 'yellow'])
plt.title('Distribution of Payment Types')
plt.show()

"""### 70 % people choose cash inplace of UPI so digital payment methods are still lagging"""

# Analysis of distribution of trip over time i.e., Hourly and daily distributions

# Plot the distribution of trips over hourly, daily, and weekly intervals
plt.figure(figsize=(15, 10))

# Hourly distribution
plt.subplot(3, 1, 1)
df['tripId'].resample('H').count().plot(kind='bar', edgecolor='black')
plt.title('Hourly Distribution of Trips')
plt.xlabel('Time')
plt.ylabel('Number of Trips')

# Daily distribution
plt.subplot(3, 1, 2)
df['tripId'].resample('D').count().plot(kind='bar', edgecolor='black')
plt.title('Daily Distribution of Trips')
plt.xlabel('Time')
plt.ylabel('Number of Trips')

# Weekly distribution
plt.subplot(3, 1, 3)
df['tripId'].resample('W').count().plot(kind='bar', edgecolor='black')
plt.title('Weekly Distribution of Trips')
plt.xlabel('Time')
plt.ylabel('Number of Trips')

plt.tight_layout()
plt.show()

# Resample data to hourly and daily intervals
hourly_distribution = df['tripId'].resample('H').count()
daily_distribution = df['tripId'].resample('D').count()

# Plot the distributions to identify peak hours and days
plt.figure(figsize=(15, 8))

# Identify peak hours
plt.subplot(2, 1, 1)
hourly_distribution.plot(kind='bar', edgecolor='black')
plt.title('Hourly Distribution of Trips')
plt.xlabel('Time (Hourly)')
plt.ylabel('Number of Trips')

# Identify peak days
plt.subplot(2, 1, 2)
daily_distribution.plot(kind='bar', edgecolor='black')
plt.title('Daily Distribution of Trips')
plt.xlabel('Time (Daily)')
plt.ylabel('Number of Trips')

plt.tight_layout()
plt.show()

"""### From above we will predict that on evening time there is peak in demands"""

# Convert 'startTime' and 'endTime' to datetime format
df['startTime'] = pd.to_datetime(df['startTime'])
df['endTime'] = pd.to_datetime(df['endTime'])

# Extract hour and day of the week from 'startTime'
df['hour'] = df['startTime'].dt.hour
df['day_of_week'] = df['startTime'].dt.day_name()

# Explore the correlation between payment types and other factors
plt.figure(figsize=(15, 8))

# Correlation between payment types and trip distance
plt.subplot(2, 2, 1)
sns.boxplot(x='paymentType', y='tripDistance', data=df)
plt.title('Correlation between Payment Types and Trip Distance')

# Correlation between payment types and trip duration
plt.subplot(2, 2, 2)
sns.boxplot(x='paymentType', y='tripDuration', data=df)
plt.title('Correlation between Payment Types and Trip Duration')

# Correlation between payment types and start hour
plt.subplot(2, 2, 3)
sns.countplot(x='hour', hue='paymentType', data=df)
plt.title('Correlation between Payment Types and Start Hour')

# Correlation between payment types and day of the week
plt.subplot(2, 2, 4)
sns.countplot(x='day_of_week', hue='paymentType', data=df, order=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])
plt.title('Correlation between Payment Types and Day of the Week')

plt.tight_layout()
plt.show()

# Preprocess and Cl

"""# Preprocess and Cleaning"""

# Missing Values

df.isnull().sum()

# Select the columns you want to normalize
columns_to_normalize = ['tripDistance', 'tripSpeed', 'tripDuration']

# Create a MinMaxScaler object
scaler = MinMaxScaler()

# Apply Min-Max scaling to selected columns
df[columns_to_normalize] = scaler.fit_transform(df[columns_to_normalize])

"""# Feature Engineering"""

# Feature engineering: Extract relevant features
df['hour'] = df['startTime'].dt.hour
df['day_of_week'] = df['startTime'].dt.dayofweek  # Monday is 0 and Sunday is 6

# Define features and target variable
features = ['hour', 'day_of_week']
target = 'tripFare'  # You may replace 'tripId' with the appropriate target variable

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, random_state=42)

# Create a preprocessing pipeline for categorical features
categorical_features = ['hour', 'day_of_week']
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(), categorical_features)
    ],
    remainder='passthrough'
)

# Create a random forest regression model
model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', RandomForestRegressor(random_state=42))
])

# Train the model
model.fit(X_train, y_train)

# Make predictions on the test set
predictions = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, predictions)
print(f'Root Mean Squared Error: {np.sqrt(mse)}')

# Example: Predict demand for a specific time
new_data = pd.DataFrame({'hour': [10], 'day_of_week': [2]})
predicted_demand = model.predict(new_data)
print(f'Predicted Demand: {predicted_demand}')

import xgboost as xgb
import numpy as np
# Assuming df is your DataFrame containing the provided data
# If not, load your data using pd.read_csv or any other appropriate method

# Feature engineering: Extract relevant features
df['hour'] = df['startTime'].dt.hour
df['day_of_week'] = df['startTime'].dt.dayofweek  # Monday is 0 and Sunday is 6

# Define features and target variable
features = ['hour', 'day_of_week']
target = 'tripFare'  # You may replace 'tripId' with the appropriate target variable

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, random_state=42)

# Create an XGBoost regressor model
model = xgb.XGBRegressor(objective ='reg:squarederror', random_state=42)

# Train the model
model.fit(X_train, y_train)

# Make predictions on the test set
predictions = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, predictions)
print(f'Root Mean Squared Error: {np.sqrt(mse)}')

# Example: Predict demand for a specific time
new_data = pd.DataFrame({'hour': [10], 'day_of_week': [2]})
predicted_demand = model.predict(new_data)
print(f'Predicted Demand: {predicted_demand}')

# Resample data to hourly intervals
hourly_demand = df['tripFare'].resample('H').count()

# Train-test split
train_size = int(len(hourly_demand) * 0.8)
train, test = hourly_demand[:train_size], hourly_demand[train_size:]

# Fit ARIMA model
order = (1, 1, 1)  # Example order, you may need to tune this
model = ARIMA(train, order=order)
fit_model = model.fit()

# Forecast
predictions = fit_model.predict(start=len(train), end=len(train) + len(test) - 1, typ='levels')

# Evaluate the model
rmse = np.sqrt(mean_squared_error(test, predictions))
print(f'Root Mean Squared Error (RMSE): {rmse}')
print("Prediction",predictions)

# Plot the results
plt.figure(figsize=(12, 6))
plt.plot(hourly_demand, label='Actual Demand')
plt.plot(predictions, label='ARIMA Forecast', color='red')
plt.title('ARIMA Model for Demand Prediction')
plt.xlabel('Time')
plt.ylabel('Number of Trips')
plt.legend()
plt.show()

#

"""## Arima is bit better as this models will give rmse vlaue will be better than rest of Random forest and XGB"""

# Resample data to hourly intervals
hourly_demand = df['tripId'].resample('H').count()

# Perform seasonal decomposition
result = seasonal_decompose(hourly_demand, model='additive', period=24)  # Assuming daily seasonality (24 hours)

# Plot the decomposed components
plt.figure(figsize=(12, 8))

plt.subplot(4, 1, 1)
plt.plot(hourly_demand, label='Original')
plt.legend()
plt.title('Original Time Series')

plt.subplot(4, 1, 2)
plt.plot(result.trend, label='Trend')
plt.legend()
plt.title('Trend Component')

plt.subplot(4, 1, 3)
plt.plot(result.seasonal, label='Seasonal')
plt.legend()
plt.title('Seasonal Component')

plt.subplot(4, 1, 4)
plt.plot(result.resid, label='Residual')
plt.legend()
plt.title('Residual Component')

plt.tight_layout()
plt.show()

"""# There is Correct indication of Seasonality in Data So we will go with Sarima Models Here"""

# Resample data to hourly intervals
hourly_demand = df['tripFare'].resample('H').count()

# Train-test split
train_size = int(len(hourly_demand) * 0.8)
train, test = hourly_demand[:train_size], hourly_demand[train_size:]

# Fit SARIMA model
order = (1, 1, 1)  # Example order, you may need to tune this
seasonal_order = (1, 1, 1, 24)  # Assuming daily seasonality (24 hours)
model = SARIMAX(train, order=order, seasonal_order=seasonal_order)
fit_model = model.fit(disp=False)

# Forecast
predictions = fit_model.get_forecast(steps=len(test))
predicted_values = predictions.predicted_mean

# Evaluate the model
rmse = np.sqrt(mean_squared_error(test, predicted_values))
print(f'Root Mean Squared Error (RMSE): {rmse}')

# Plot the results
plt.figure(figsize=(12, 6))
plt.plot(hourly_demand, label='Actual Demand')
plt.plot(predicted_values, label='SARIMA Forecast', color='red')
plt.title('SARIMA Model for Demand Prediction')
plt.xlabel('Time')
plt.ylabel('Number of Trips')
plt.legend()
plt.show()

"""### Best model we will get is Sarima models"""